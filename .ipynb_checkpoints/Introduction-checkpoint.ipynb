{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_chars : 162881\n",
      "num_words : 37360\n",
      "num_sents : 3106\n",
      "[u'To', u'be', u',', u'or', u'not', u'to', u'be', u',', u'that', u'is', u'the', u'Question', u':', u'Whether', u\"'\", u'tis', u'Nobler', u'in', u'the', u'minde', u'to', u'suffer', u'The', u'Slings', u'and', u'Arrowes', u'of', u'outragious', u'Fortune', u',', u'Or', u'to', u'take', u'Armes', u'against', u'a', u'Sea', u'of', u'troubles', u',', u'And', u'by', u'opposing', u'end', u'them', u':', u'to', u'dye', u',', u'to', u'sleepe', u'No', u'more', u';', u'and', u'by', u'a', u'sleepe', u',', u'to', u'say', u'we', u'end', u'The', u'Heart', u'-', u'ake', u',', u'and', u'the', u'thousand', u'Naturall', u'shockes', u'That', u'Flesh', u'is', u'heyre', u'too', u'?']\n"
     ]
    }
   ],
   "source": [
    "hamlet_id = 'shakespeare-hamlet.txt'\n",
    "print \"num_chars :\",len(gutenberg.raw(hamlet_id))\n",
    "print \"num_words :\",len(gutenberg.words(hamlet_id))\n",
    "print \"num_sents :\",len(gutenberg.sents(hamlet_id))\n",
    "hamlet_sents = nltk.corpus.gutenberg.sents(hamlet_id)\n",
    "print hamlet_sents[1303]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Conversion and Replacements\n",
    "To parse texts where Upper-case and Lower-case does not have much significance. <br>\n",
    "Alternatively, if there a multiple patterns of the same word with varying cases.\n",
    "\n",
    "Consider the following text : \n",
    "> \"The **General Data Protection Regulation** (EU) 2016/679 is a regulation in EU law on data protection and privacy for all individuals within the European Union and the European Economic Area. **Gdpr** also addresses the export of personal data outside the EU and EEA. The **GDPR** aims primarily to give control to citizens and residents over their personal data and to simplify the regulatory environment for international business by unifying the regulation within the EU.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 'GDPR' only :  ['GDPR']\n",
      "Checking for 'GDPR' irrespective of case :  ['gdpr', 'gdpr']\n",
      "Checking for 'GDPR' irrespective of case after replacing abbreviations :  ['gdpr', 'gdpr', 'gdpr']\n"
     ]
    }
   ],
   "source": [
    "sample = \"The General Data Protection Regulation (EU) 2016/679 is a regulation in EU law on data protection and privacy for all individuals within the European Union and the European Economic Area. Gdpr also addresses the export of personal data outside the EU and EEA. The GDPR aims primarily to give control to citizens and residents over their personal data and to simplify the regulatory environment for international business by unifying the regulation within the EU.\"\n",
    "print \"Checking for 'GDPR' only : \",[i for i in sample.split(' ') if i == 'GDPR' ]\n",
    "print \"Checking for 'GDPR' irrespective of case : \",[i for i in sample.lower().split(' ') if i == 'gdpr' ]\n",
    "sample_rep = sample.replace('General Data Protection Regulation','GDPR')\n",
    "print \"Checking for 'GDPR' irrespective of case after replacing abbreviations : \",[i for i in sample_rep.lower().split(' ') if i == 'gdpr' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation : NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this's a sent tokenize test.\",\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " \"Now it's your turn.\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sample2 = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "sent_tokenize(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', \"'s\", 'a', 'sent', 'tokenize', 'test', '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenising the first sentence of sample2\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(sent_tokenize(sample2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation : Spacy\n",
    "Note : \n",
    "1. Unicode\n",
    "2. Single function\n",
    "3. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this DET this\n",
      "'s VERB be\n",
      "a DET a\n",
      "sent ADJ sent\n",
      "tokenize NOUN tokenize\n",
      "test NOUN test\n",
      ". PUNCT .\n",
      "====\n",
      "this DET this\n",
      "is VERB be\n",
      "sent VERB send\n",
      "two NUM two\n",
      ". PUNCT .\n",
      "====\n",
      "is VERB be\n",
      "this DET this\n",
      "sent VERB send\n",
      "three NUM three\n",
      "? PUNCT ?\n",
      "====\n",
      "sent VERB send\n",
      "4 NUM 4\n",
      "is VERB be\n",
      "cool ADJ cool\n",
      "! PUNCT !\n",
      "====\n",
      "Now ADV now\n",
      "it PRON -PRON-\n",
      "'s VERB be\n",
      "your ADJ -PRON-\n",
      "turn NOUN turn\n",
      ". PUNCT .\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for s in x.sents:\n",
    "    for w in s:\n",
    "        print w,w.pos_,w.lemma_\n",
    "    print \"====\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging : NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('gon', 'VBG'),\n",
       " ('na', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('him', 'PRP'),\n",
       " ('an', 'DT'),\n",
       " ('offer', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('refuse', 'VB')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I'm gonna make him an offer he can't refuse\"\n",
    "words = nltk.word_tokenize(sent)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presumably --> presum\n",
      "presume --> presum\n",
      "presuming --> presum\n",
      "presumed --> presum\n",
      "presumes --> presum\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"presumably\",\"presume\",\"presuming\",\"presumed\",\"presumes\"]\n",
    "for w in words:\n",
    "    print w,\"-->\",porter_stemmer.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing without POS Tags\n",
      "presumably --> presumably\n",
      "presume --> presume\n",
      "presuming --> presuming\n",
      "presumed --> presumed\n",
      "presumes --> presumes\n",
      "\n",
      "Lemmatizing with POS Tags\n",
      "presumably --> presumably\n",
      "presume --> presume\n",
      "presuming --> presume\n",
      "presumed --> presume\n",
      "presumes --> presume\n",
      "\n",
      "\n",
      "aardwolves --lemma--> aardwolf\n",
      "aardwolves --stem--> aardwolv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = [\"presumably\",\"presume\",\"presuming\",\"presumed\",\"presumes\"]\n",
    "pos_tags = [\"a\",\"v\",\"v\",\"v\",\"v\"]\n",
    "print \"Lemmatizing without POS Tags\"\n",
    "for w in words:\n",
    "    print w,\"-->\",wordnet_lemmatizer.lemmatize(w)\n",
    "\n",
    "print \"\\nLemmatizing with POS Tags\"\n",
    "for w,p in zip(words,pos_tags):\n",
    "    print w,\"-->\",wordnet_lemmatizer.lemmatize(w, pos=p)\n",
    "\n",
    "print \"\\n\\n\",'aardwolves','--lemma-->',wordnet_lemmatizer.lemmatize('aardwolves')\n",
    "print 'aardwolves','--stem-->',porter_stemmer.stem('aardwolves')\n",
    "#     print wordnet_lemmatizer.lemmatize('presuming', pos='v')\n",
    "# wordnet_lemmatizer.lemmatize('are', pos='v')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'aardwolf'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
